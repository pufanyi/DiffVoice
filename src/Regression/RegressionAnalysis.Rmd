---
title: "RegressionAnalysis"
author: "qy"
knit: (function(inputFile, encoding) {
      out_dir <- "../docs/age";
      rmarkdown::render(inputFile,
                        encoding=encoding,
                        output_dir=file.path(dirname(inputFile), out_dir))})
output:
  html_document:
    theme: journal
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Preparation

Here we want to analyze the difference of voices between genders. 

```{r}
voice = read.csv("../../data/original/train.csv")
X <- voice[, !(colnames(voice) %in% c("gender"))]
y <- voice$gender
y <- ifelse(y == "male", 0, 1)
head(voice)
```

```{r}
# install.packages("caret")
library(caret)
set.seed(42)

# Split the data into training and testing sets
train_indices <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_indices, ]
X_test <- X[-train_indices, ]
y_train <- y[train_indices]
y_test <- y[-train_indices]

```

## Correlation

We select the following columns for correlation analysis, as we assume they would highly correlate with each other. We plot the heatmap as follows:


```{r}
# install.packages("corrplot")
library(corrplot)

distribution_cols <- c("meanfreq", "median", "Q25", "Q75", "meandom")

correlation_matrix <- cor(voice[, distribution_cols])

corrplot(correlation_matrix, method = "color", type = "upper", 
         addCoef.col = "black", tl.col = "black", tl.srt = 45,
         diag = FALSE)

```

```{r}
# install.packages("GGally")
library(GGally)
library(ggplot2)

# Save the plot to a PDF file
# pdf("../../docs/report/graphs/scatter_matrix.pdf", width = 7, height = 7)
# ggpairs(voice, columns = distribution_cols, 
        #mapping = ggplot2::aes(color = gender),
       # lower = list(continuous = wrap("points", size = 0.1)),
       # diag = list(continuous = wrap("barDiag", size = 0.1)))
# dev.off()
```


## Simple Linear Regression

We first perform simple linear regression

### Relationship between mean frequency and meanfun

```{r}
cor.test(voice$meanfreq, voice$meanfun)
```


```{r}
model1 <- lm(voice$meanfreq ~ voice$meanfun)
summary(model1)
model1$coefficients
confint(model1, level = 0.95)
```

To viualise the relationship between mean frequency and meanfun, we plot the scatter plot as follows:

```{r}
plot(voice$meanfun, voice$meanfreq, 
     xlab = "Mean Fundamental Frequency", ylab = "Mean Frequency",
     main = "Scatter Plot with Regression Line")
abline(model1, col = "red")
```

### Relationship between mean frequency and meandom

```{r}
cor.test(voice$meanfreq, voice$meandom)
```

```{r}
model2 <- lm(voice$meanfreq ~ voice$meandom)
summary(model2)
model2$coefficients
confint(model2, level = 0.95)
```

To viualise the relationship between mean frequency and meandom, we plot the scatter plot as follows:

```{r}
plot(voice$meandom, voice$meanfreq, 
     xlab = "Meandom", ylab = "Mean Frequency",
     main = "Scatter Plot with Regression Line")
abline(model2, col = "red")
```

### Relationship between mean frequency and mindom

```{r}
cor.test(voice$meanfreq, voice$mindom)
```

```{r}
model3 <- lm(voice$meanfreq ~ voice$mindom)
summary(model3)
model3$coefficients
confint(model3, level = 0.95)
```

To viualise the relationship between mean frequency and mindom, we plot the scatter plot as follows:

```{r}
plot(voice$mindom, voice$meanfreq, 
     xlab = "Mindom", ylab = "Mean Frequency",
     main = "Scatter Plot with Regression Line")
abline(model3, col = "red")
```


### Relationship between mean frequency and maxdom

```{r}
cor.test(voice$meanfreq, voice$maxdom)
```

```{r}
model4 <- lm(voice$meanfreq ~ voice$maxdom)
summary(model4)
model4$coefficients
confint(model4, level = 0.95)
```

To viualise the relationship between mean frequency and maxdom, we plot the scatter plot as follows:

```{r}
plot(voice$maxdom, voice$meanfreq, 
     xlab = "Maxdom", ylab = "Mean Frequency",
     main = "Scatter Plot with Regression Line")
abline(model4, col = "red")
```

```{r}
simple_linear_regression_plot <- function(X, y, caption) {
  model <- lm(y ~ X)
  plot(X, y, xlab = names(X), ylab = names(y), main = caption)
  abline(model, col = "red")
}
```

```{r}
pdf("../../docs/report/graphs/simple_linear_regression_plot.pdf", width = 9, height = 6)

par(mfrow = c(2, 2))
simple_linear_regression_plot(voice$meanfun, voice$meanfreq, "Mean Frequency vs Mean Fun")
simple_linear_regression_plot(voice$meanfun, voice$meandom, "Mean Frequency vs Mean Dom")
simple_linear_regression_plot(voice$meanfun, voice$mindom, "Mean Frequency vs Min Dom")
simple_linear_regression_plot(voice$meanfun, voice$maxdom, "Mean Frequency vs Max Dom")

dev.off()
```



## Multiple Linear Regression

We then perform multiple linear regression, finding the relationship between mean frequency and meanfun, meandom, mindom, and maxdom.

```{r}
model <- lm(meanfreq ~ meanfun + meandom + mindom + maxdom, data = X_train)
step(model, direction="backward")
```

```{r}
summary(model)
```


```{r}
predicted <- predict(model, newdata = X_test)
```


```{r}
pdf("../../docs/report/graphs/multiple_lr.pdf", width = 6, height = 6)

plot(predicted, X_test$meanfreq, xlab = "Predicted", ylab = "Actual", main = "Predicted vs Actual")
abline(0, 1, col = "red")
dev.off()
```


```{r}
voice = read.csv("../../data/gender/balanced_train.csv")
X <- voice[, !(colnames(voice) %in% c("gender"))]
y <- voice$gender
y <- ifelse(y == "male", 0, 1)
head(voice)
```

```{r}
# install.packages("caret")
library(caret)
set.seed(42)

# Split the data into training and testing sets
train_indices <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_indices, ]
X_test <- X[-train_indices, ]
y_train <- y[train_indices]
y_test <- y[-train_indices]

```



## Logistic Regression

Logistic regression models the probability that Gender belong to "male" or "female". 


```{r}
model <- glm(y_train ~ ., data = cbind(y_train, X_train), family = binomial)

print(paste("beta_0:", coef(model)[1]))
print(paste("beta_1:", coef(model)[2]))

train_pred <- predict(model, newdata = X_train, type = "response")
train_accuracy <- mean(ifelse(train_pred > 0.5, 1, 0) == y_train)
print(paste("Training set accuracy:", train_accuracy))

test_pred <- predict(model, newdata = X_test, type = "response")
test_accuracy <- mean(ifelse(test_pred > 0.5, 1, 0) == y_test)
print(paste("Test set accuracy:", test_accuracy))

conf_matrix <- table(ifelse(test_pred > 0.5, 1, 0), y_test)
print(conf_matrix)

TPR_logreg_1 <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
print(paste("True Positive Rate:", TPR_logreg_1))

TNR_logreg_1 <- conf_matrix[1, 1] / sum(conf_matrix[1, ])
print(paste("True Negative Rate:", TNR_logreg_1))

FPR_logreg_1 <- conf_matrix[1, 2] / sum(conf_matrix[1, ])
print(paste("False Positive Rate:", FPR_logreg_1))

FNR_logreg_1 <- conf_matrix[2, 1] / sum(conf_matrix[2, ])
print(paste("False Negative Rate:", FNR_logreg_1))
```




## K-Nearest Neighbors



```{r}
library(class)
library(caret)

knn_model <- knn(train = X_train, test = X_test, cl = y_train, k = 3)

test_accuracy <- sum(knn_model == y_test) / length(y_test)
print(paste("Test set accuracy:", test_accuracy))

conf_matrix <- table(Actual = y_test, Predicted = knn_model)
print(conf_matrix)

TP <- conf_matrix[2, 2]  # True Positives
TN <- conf_matrix[1, 1]  # True Negatives
FP <- conf_matrix[1, 2]  # False Positives
FN <- conf_matrix[2, 1]  # False Negatives

TPR_knn <- TP / (TP + FN)  
TNR_knn <- TN / (TN + FP)  
FPR_knn <- FP / (FP + TN)  
FNR_knn <- FN / (FN + TP)  

print(paste("True Positive Rate:", TPR_knn))
print(paste("True Negative Rate:", TNR_knn))
print(paste("False Positive Rate:", FPR_knn))
print(paste("False Negative Rate:", FNR_knn))
```

```{r}
# Create a matrix of the rates
rates_matrix_1 <- matrix(c(TNR_logreg_1, FPR_logreg_1, FNR_logreg_1, TPR_logreg_1), nrow = 2)

# Add dimnames for clarity
dimnames(rates_matrix_1) <- list(c("Actual Negative", "Actual Positive"),
                                c("Predicted Negative", "Predicted Positive"))

# Convert the matrix to a data frame for ggplot2
rates_df_1 <- as.data.frame(as.table(rates_matrix_1))

# Load ggplot2
library(ggplot2)
library(gridExtra)

# Create a matrix of the rates
rates_matrix_2 <- matrix(c(TNR_knn, FPR_knn, FNR_knn, TPR_knn), nrow = 2, byrow = TRUE)

# Add dimnames for clarity
dimnames(rates_matrix_2) <- list(c("Actual Negative", "Actual Positive"),
                                c("Predicted Negative", "Predicted Positive"))

# Convert the matrix to a data frame for ggplot2
rates_df_2 <- as.data.frame(as.table(rates_matrix))

# Create the first heatmap plot
heatmap1 <- ggplot(data = rates_df_1, aes(x = Var2, y = Var1, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "blue") +
  geom_text(aes(label = sprintf("%.2f", Freq)), vjust = 1) +
  labs(x = "Predicted", y = "Actual", fill = "Rate") +
  ggtitle("Confusion Matrix for Logistics Regression") +
  theme_minimal()

# Create the second heatmap plot (this could be a different heatmap)
heatmap2 <- ggplot(data = rates_df_2, aes(x = Var2, y = Var1, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "blue") +
  geom_text(aes(label = sprintf("%.2f", Freq)), vjust = 1) +
  labs(x = "Predicted", y = "Actual", fill = "Rate") +
  ggtitle("Confusion Matrix for KNN") +
  theme_minimal()

# Save the side-by-side heatmaps to a PDF file
pdf("../../docs/report/graphs/heatmaps.pdf", width = 10, height = 4)
grid.arrange(heatmap1, heatmap2, ncol = 2)
dev.off()
```

